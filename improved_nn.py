# -*- coding: utf-8 -*-
"""improved_NN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cVNHRnn0LrUyuZt-SoVxrohWDMgqfgjQ
"""

import numpy as np
import pandas as pd

training_data = pd.read_csv('df_train.csv')
test_data = pd.read_csv('df_test.csv')

training_data.head()

import numpy as np
import pandas as pd

class NN(object):
    def __init__(self, input_size=10, hidden_units=11, output_size=1):
        self.input_size = input_size
        self.output_size = output_size
        self.hidden_units = hidden_units

        # Initialize weights and biases
        np.random.seed(1)
        self.w1 = np.random.randn(self.input_size, self.hidden_units) * np.sqrt(2./self.input_size)
        self.w2 = np.random.randn(self.hidden_units, self.output_size) * np.sqrt(2./self.hidden_units)
        self.b1 = np.zeros((1, self.hidden_units))
        self.b2 = np.zeros((1, self.output_size))

        # Initialize momentum terms
        self.v_w1 = np.zeros_like(self.w1)
        self.v_w2 = np.zeros_like(self.w2)
        self.v_b1 = np.zeros_like(self.b1)
        self.v_b2 = np.zeros_like(self.b2)

        # Initialize scaling parameters
        self.feature_means = None
        self.feature_stds = None

    def _scale_features(self, X, fit=False):
        """
        Custom feature scaling implementation (standardization)
        """
        if fit:
            self.feature_means = np.mean(X, axis=0)
            self.feature_stds = np.std(X, axis=0)
            # Handle zero standard deviation
            self.feature_stds = np.where(self.feature_stds == 0, 1, self.feature_stds)

        if self.feature_means is None or self.feature_stds is None:
            raise ValueError("Scaling parameters not fitted. Call train first.")

        return (X - self.feature_means) / self.feature_stds

    def save_model(self, w1_file='w1.npy', w2_file='w2.npy', b1_file='b1.npy', b2_file='b2.npy',
                  means_file='means.npy', stds_file='stds.npy'):
        np.save(w1_file, self.w1)
        np.save(w2_file, self.w2)
        np.save(b1_file, self.b1)
        np.save(b2_file, self.b2)
        np.save(means_file, self.feature_means)
        np.save(stds_file, self.feature_stds)
        print("Model weights, biases, and scaling parameters saved successfully.")

    def load_model(self, w1_file='w1.npy', w2_file='w2.npy', b1_file='b1.npy', b2_file='b2.npy',
                  means_file='means.npy', stds_file='stds.npy'):
        self.w1 = np.load(w1_file)
        self.w2 = np.load(w2_file)
        self.b1 = np.load(b1_file)
        self.b2 = np.load(b2_file)
        self.feature_means = np.load(means_file)
        self.feature_stds = np.load(stds_file)
        print("Model weights, biases, and scaling parameters loaded successfully.")

    def _forward_propagation(self, X):
        # Store intermediary values for backpropagation
        self.z1 = np.dot(X, self.w1) + self.b1
        self.a1 = self._relu(self.z1)
        self.z2 = np.dot(self.a1, self.w2) + self.b2
        self.a2 = self._sigmoid(self.z2)
        return self.a2

    def _sigmoid(self, z):
        z = np.clip(z, -500, 500)  # Prevent overflow
        return 1 / (1 + np.exp(-z))

    def _relu(self, z):
        return np.maximum(0, z)

    def _relu_derivative(self, z):
        return (z > 0).astype(float)

    def _loss(self, predict, y):
        m = y.shape[0]
        epsilon = 1e-15  # Small constant to prevent log(0)
        predict = np.clip(predict, epsilon, 1 - epsilon)
        loss = -np.mean(y * np.log(predict) + (1 - y) * np.log(1 - predict))
        return loss

    def _backward_propagation(self, X, y, learning_rate=0.01, momentum=0.9):
        m = X.shape[0]

        # Output layer
        dz2 = self.a2 - y
        dw2 = np.dot(self.a1.T, dz2) / m
        db2 = np.sum(dz2, axis=0, keepdims=True) / m

        # Hidden layer
        dz1 = np.dot(dz2, self.w2.T) * self._relu_derivative(self.z1)
        dw1 = np.dot(X.T, dz1) / m
        db1 = np.sum(dz1, axis=0, keepdims=True) / m

        # Update with momentum
        self.v_w1 = momentum * self.v_w1 + learning_rate * dw1
        self.v_w2 = momentum * self.v_w2 + learning_rate * dw2
        self.v_b1 = momentum * self.v_b1 + learning_rate * db1
        self.v_b2 = momentum * self.v_b2 + learning_rate * db2

        self.w1 -= self.v_w1
        self.w2 -= self.v_w2
        self.b1 -= self.v_b1
        self.b2 -= self.v_b2

    def train(self, X, y, iterations=100, learning_rate=0.01, momentum=0.9, batch_size=32):
        # Convert inputs to numpy arrays if they're pandas objects
        X = X.values if isinstance(X, pd.DataFrame) else X
        y = y.values if isinstance(y, pd.Series) else y
        y = y.reshape(-1, 1)

        # Fit and transform the features
        X_scaled = self._scale_features(X, fit=True)

        # Initialize best weights
        best_loss = float('inf')
        best_weights = None

        n_samples = X_scaled.shape[0]
        for i in range(iterations):
            # Mini-batch training
            for j in range(0, n_samples, batch_size):
                batch_X = X_scaled[j:j+batch_size]
                batch_y = y[j:j+batch_size]

                # Forward and backward pass
                y_hat = self._forward_propagation(batch_X)
                self._backward_propagation(batch_X, batch_y, learning_rate, momentum)

            # Calculate loss on full dataset
            full_predictions = self._forward_propagation(X_scaled)
            current_loss = self._loss(full_predictions, y)

            # Save best weights
            if current_loss < best_loss:
                best_loss = current_loss
                best_weights = {
                    'w1': self.w1.copy(),
                    'w2': self.w2.copy(),
                    'b1': self.b1.copy(),
                    'b2': self.b2.copy()
                }

            if i % 10 == 0:
                print(f"Iteration {i}, Loss: {current_loss:.4f}")

        # Restore best weights
        self.w1 = best_weights['w1']
        self.w2 = best_weights['w2']
        self.b1 = best_weights['b1']
        self.b2 = best_weights['b2']

    def predict(self, X):
        # Convert input to numpy array if it's a pandas object
        X = X.values if isinstance(X, pd.DataFrame) else X

        # Scale the features
        X_scaled = self._scale_features(X)

        # Make prediction
        y_hat = self._forward_propagation(X_scaled)
        return (y_hat >= 0.5).astype(int)

    def score(self, y_pred, y_true):
        if isinstance(y_true, pd.Series):
            y_true = y_true.values
        y_true = y_true.reshape(-1, 1)

        accuracy = np.mean(y_pred == y_true) * 100

        # Calculate additional metrics
        tp = np.sum((y_pred == 1) & (y_true == 1))
        fp = np.sum((y_pred == 1) & (y_true == 0))
        tn = np.sum((y_pred == 0) & (y_true == 0))
        fn = np.sum((y_pred == 0) & (y_true == 1))

        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

        return {
            'accuracy': accuracy,
            'precision': precision * 100,
            'recall': recall * 100,
            'f1_score': f1 * 100
        }

if __name__=='__main__':
    # Example usage
    train_X = training_data[['use_of_ip', 'count.', '@ Precence', '- Precence', '∼ Precence',
                            'count_embed_domian', 'sus_url', 'short_url', 'HTTPS in Domain', 'url_length']]
    test_X = test_data[['use_of_ip', 'count.', '@ Precence', '- Precence', '∼ Precence',
                        'count_embed_domian', 'sus_url', 'short_url', 'HTTPS in Domain', 'url_length']]
    train_y = training_data['label_code']
    test_y = test_data['label_code']

    clr = NN()
    clr.train(train_X, train_y, iterations=100, learning_rate=0.01, batch_size=32)
    pred_y = clr.predict(test_X)
    metrics = clr.score(pred_y, test_y)

    print("\nModel Performance:")
    for metric, value in metrics.items():
        print(f"{metric.title()}: {value:.2f}%")

    clr.save_model()
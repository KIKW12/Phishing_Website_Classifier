# -*- coding: utf-8 -*-
"""improved_NN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cVNHRnn0LrUyuZt-SoVxrohWDMgqfgjQ
"""

import numpy as np
import pandas as pd

training_data = pd.read_csv('df_train.csv')
test_data = pd.read_csv('df_test.csv')

training_data.head()


#neural network class
class NN():
    def __init__(self, input_size=10, hidden_units=11, output_size=1):
        self.input_size = input_size
        self.output_size = output_size
        self.hidden_units = hidden_units

        #random weights and biases
        np.random.seed(1) #random values will be the same each time I run it
        self.w1 = np.random.randn(self.input_size, self.hidden_units) * np.sqrt(2./self.input_size) #m*n matrix with "He initialization"
        self.w2 = np.random.randn(self.hidden_units, self.output_size) * np.sqrt(2./self.hidden_units) #m*n matrix with "He initialization"
        self.b1 = np.zeros((1, self.hidden_units)) #bias vector, initially 0
        self.b2 = np.zeros((1, self.output_size)) #bias vector, initially 0

        #momentum matrices (to accumulate past gradient)
        #allow the network to converge faster
        self.v_w1 = np.zeros_like(self.w1)
        self.v_w2 = np.zeros_like(self.w2)
        self.v_b1 = np.zeros_like(self.b1)
        self.v_b2 = np.zeros_like(self.b2)



        #scaling parameters to normalize the features for gradient descent
        #these values are placeholders until they’re calculated
        self.feature_means = None #mean of each feature
        self.feature_stds = None #standard deviation of teach feature


    #function to standardize features
    def _scale_features(self, X, fit=False):
        '''Custom feature scaling implementation (standardization)
        Remember that differences in scale can lead to an uneven optimization process,
        where larger values dominate the learning process'''

        if fit:
            self.feature_means = np.mean(X, axis=0) #mean of each feature along each column
            self.feature_stds = np.std(X, axis=0) #standard deviation of each feature along each column
            self.feature_stds = np.where(self.feature_stds == 0, 1, self.feature_stds) #handle zero standard deviation

        return (X - self.feature_means) / self.feature_stds #subtracting the mean and dividing by the standard deviation for that feature




    #sigmoid activation function
    def _sigmoid(self, z):
        z = np.clip(z, -500, 500)  #to avoid overflow
        return 1 / (1 + np.exp(-z))

    #ReLU activation function
    def _relu(self, z):
        return np.maximum(0, z)

    #derivative of ReLU
    def _relu_derivative(self, z):
        return (z > 0).astype(float)


    def _forward_propagation(self, X):
        self.z1 = np.dot(X, self.w1) + self.b1 #matrix multiplication between the input data and w1 and bias
        self.a1 = self._relu(self.z1) #applies ReLU
        self.z2 = np.dot(self.a1, self.w2) + self.b2 # multiplication of activated output from the hidden layer
        self.a2 = self._sigmoid(self.z2) #applies simoig
        return self.a2 #returns a probability between 0 and 1

    #binary cross-entropy
    def _loss(self, predict, y):
        m = y.shape[0]
        epsilon = 1e-15  # Small constant to prevent log(0)
        predict = np.clip(predict, epsilon, 1 - epsilon)
        loss = -np.mean(y * np.log(predict) + (1 - y) * np.log(1 - predict))
        return loss

    def _backward_propagation(self, X, y, learning_rate=0.01, momentum=0.9):
        m = X.shape[0]

        #output layer
        dz2 = self.a2 - y
        dw2 = np.dot(self.a1.T, dz2) / m
        db2 = np.sum(dz2, axis=0, keepdims=True) / m

        #hidden layer
        dz1 = np.dot(dz2, self.w2.T) * self._relu_derivative(self.z1)
        dw1 = np.dot(X.T, dz1) / m
        db1 = np.sum(dz1, axis=0, keepdims=True) / m

        #update with momentum
        self.v_w1 = momentum * self.v_w1 + learning_rate * dw1
        self.v_w2 = momentum * self.v_w2 + learning_rate * dw2
        self.v_b1 = momentum * self.v_b1 + learning_rate * db1
        self.v_b2 = momentum * self.v_b2 + learning_rate * db2

        self.w1 -= self.v_w1
        self.w2 -= self.v_w2
        self.b1 -= self.v_b1
        self.b2 -= self.v_b2

    def train(self, X, y, iterations=100, learning_rate=0.01, momentum=0.9, batch_size=32):
        X = X.values if isinstance(X, pd.DataFrame) else X #convert inputs to numpy arrays if they're dataframes
        y = y.values if isinstance(y, pd.Series) else y #convert inputs to numpy arrays if they're dataframes
        y = y.reshape(-1, 1) #reshape array with 1 column and as many rows as necessary 

        #transform the features
        X_scaled = self._scale_features(X, fit=True)

        #track the best model weights and lowest loss encountered during training
        best_loss = float('inf') #initially infinity, updates to the lowest in each iteration
        best_weights = None #its going to be a dictionary

        n_samples = X_scaled.shape[0] #number of data points in the transformed dataset
        for i in range(iterations):
            
            #mini-batch training
            for j in range(0, n_samples, batch_size):
                batch_X = X_scaled[j:j+batch_size]
                batch_y = y[j:j+batch_size]

                y_hat = self._forward_propagation(batch_X)
                self._backward_propagation(batch_X, batch_y, learning_rate, momentum)

            #calculate loss on full dataset
            full_predictions = self._forward_propagation(X_scaled)
            current_loss = self._loss(full_predictions, y)

            #save best weights and biases
            if current_loss < best_loss:
                best_loss = current_loss
                best_weights = {
                    'w1': self.w1.copy(),
                    'w2': self.w2.copy(),
                    'b1': self.b1.copy(),
                    'b2': self.b2.copy()}

            if i % 10 == 0:
                print(f"Iteration {i}, Loss: {current_loss:.4f}")

        '''restore best weights and biases. This way we make sure that we are
        not left with a version of the model that might have had an 
        increase in loss right at the end''' 
        self.w1 = best_weights['w1']
        self.w2 = best_weights['w2']
        self.b1 = best_weights['b1']
        self.b2 = best_weights['b2']

    def predict(self, X):
        X = X.values if isinstance(X, pd.DataFrame) else X #convert to np array

        #scale features
        X_scaled = self._scale_features(X)

        #make prediction
        y_hat = self._forward_propagation(X_scaled)
        return (y_hat >= 0.5).astype(int)

    def accuracy_measurement(self, y_pred, y_true):
        if isinstance(y_true, pd.Series):
            y_true = y_true.values
        y_true = y_true.reshape(-1, 1)

        accuracy = np.mean(y_pred == y_true) * 100

        return accuracy


    def save_model(self, w1_file='w1.npy', w2_file='w2.npy', b1_file='b1.npy', b2_file='b2.npy', means_file='means.npy', stds_file='stds.npy'):
        np.save(w1_file, self.w1)
        np.save(w2_file, self.w2)
        np.save(b1_file, self.b1)
        np.save(b2_file, self.b2)
        np.save(means_file, self.feature_means)
        np.save(stds_file, self.feature_stds)

    def load_model(self, w1_file='w1.npy', w2_file='w2.npy', b1_file='b1.npy', b2_file='b2.npy', means_file='means.npy', stds_file='stds.npy'):
        self.w1 = np.load(w1_file)
        self.w2 = np.load(w2_file)
        self.b1 = np.load(b1_file)
        self.b2 = np.load(b2_file)
        self.feature_means = np.load(means_file)
        self.feature_stds = np.load(stds_file)

if __name__=='__main__':
    train_X = training_data[['use_of_ip', 'count.', '@ Precence', '- Precence', '∼ Precence', 'count_embed_domian', 'sus_url', 'short_url', 'HTTPS in Domain', 'url_length']]
    
    test_X = test_data[['use_of_ip', 'count.', '@ Precence', '- Precence', '∼ Precence', 'count_embed_domian', 'sus_url', 'short_url', 'HTTPS in Domain', 'url_length']]
    
    train_y = training_data['label_code']
    test_y = test_data['label_code']

    mlp_classifier = NN()
    mlp_classifier.train(train_X, train_y, iterations=100, learning_rate=0.01, batch_size=32)
    pred_y = mlp_classifier.predict(test_X)
    metrics = mlp_classifier.accuracy_measurement(pred_y, test_y)

    print("\nModel Performance:")
    print(metrics)

    mlp_classifier.save_model()